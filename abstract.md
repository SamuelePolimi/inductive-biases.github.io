---
published: false
---

Over the last decade, deep networks have propelled machine learning to accomplish tasks previously considered far out of reach, human-level performance in image classification and game-playing.
However, research has also shown that the deep networks are often brittle to distributional shifts in data: it has been shown that human-imperceptible changes can lead to absurd predictions. 
In many application areas, including physics, robotics, social sciences and life sciences, this motivates the need for robustness and interpretability, so that models can be trusted in practical applications. 
Interpretable and robust models can be constructed by incorporating prior knowledge within the model or learning process as an inductive bias, thereby avoiding 
overfitting and making the model easier to understand for scientists and non-machine-learning experts.
In this workshop, we bring together researchers from different application areas to study the inductive biases that can be used to obtain interpretable models.
We invite speakers from physics, robotics, and other related areas to share their ideas and success stories on inductive biases.
We also invite researchers to submit extended abstracts for contributed talks and posters to initiate lively discussion on inductive biases and foster this growing community of researchers.